from kafka import KafkaConsumer
import time
import pickle
import numpy as np
import pandas as pd
from io import StringIO

# T·∫£i m√¥ h√¨nh t·ª´ t·ªáp pickle
with open("random_forest_model.pkl", 'rb') as file:
    model = pickle.load(file)

# Khai b√°o th√¥ng tin kafka
consumer = KafkaConsumer(
    'network_attack',
    bootstrap_servers=['localhost:9092'],
    auto_offset_reset='earliest',
    enable_auto_commit=False,
    group_id='your_consumer_group'
)

# C·∫•u h√¨nh tham s·ªë kafka
batch_size = 10 # ƒê·ªß 10 b·∫£n ghi l√† b·∫Øt ƒë·∫ßu x·ª≠ l√Ω
timeout_readkafka = 5000 # ƒê·ª£i b·∫£n ghi tr√™n kafka 5 gi√¢y
batch_wait_time = 60000 # 60s gi√¢y, ƒë·ª£i 60 gi√¢y m√† ko ƒë·ªß 10 b·∫£n ghi th√¨ c√≥ bao nhi√™u predict b√¢y nhi·ªÅu
hack_threshold = 0.5

column_names = [
    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',
    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',
    'num_compromised', 'root_shell', 'su_attempted', 'num_root',
    'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',
    'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',
    'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate',
    'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',
    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',
    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack', 'level'
]


def get_train_encoded_features():
    train_file_small = 'KDDTrain+.txt'
    df = pd.read_csv(train_file_small)

    df.columns = column_names
    df['attack_flag'] = df['attack'].apply(lambda x: 0 if x == 'normal' else 1)

    # M√£ h√≥a c√°c ƒë·∫∑c tr∆∞ng ph√¢n lo·∫°i
    categorical_features = ['protocol_type', 'service', 'flag']
    encoded_features = pd.get_dummies(df[categorical_features])

    # # C√°c ƒë·∫∑c tr∆∞ng s·ªë
    # numeric_features = ['duration', 'src_bytes', 'dst_bytes']
    # # Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh
    # dataset_prepared = encoded_features.join(df[numeric_features])
    return encoded_features  # dataset_prepared, df, encoded_features


encoded_features = get_train_encoded_features()

def process_incoming_df(income_df, train_encoded_features):
    # Assign columns names
    income_df.columns = column_names

    # Assign attach_flag
    income_df['attack_flag'] = 0

    # One hot transform
    categorical_features = ['protocol_type', 'service', 'flag']
    test_encoded_base = pd.get_dummies(income_df[categorical_features])

    # Add th√™m c√°c c·ªôt gi√° tr·ªã kh√¥ng c√≥ trong test (d·ª±a v√†o full data c·ªßa train)
    test_index = np.arange(len(income_df))
    missing_columns = list(set(train_encoded_features.columns) - set(test_encoded_base.columns))
    test_missing_df = pd.DataFrame(0, index=test_index, columns=missing_columns)
    test_encoded_aligned = test_encoded_base.join(test_missing_df)
    test_encoded_final = test_encoded_aligned[train_encoded_features.columns].fillna(0)

    # Th√™m C√°c ƒë·∫∑c tr∆∞ng s·ªë
    numeric_features = ['duration', 'src_bytes', 'dst_bytes']
    test_data_prepared = test_encoded_final.join(income_df[numeric_features])

    return test_data_prepared

def process_batch(batch):
    # L·∫∑p qua t·ª´ng message trong batch
    df_string = ""
    for i_message in batch:
        df_string += str(i_message.value.decode("utf-8"))

    print("DF String =" , df_string)

    df = pd.read_csv(StringIO(df_string), sep=",", header=None)
    print("S·ªë l∆∞·ª£ng b·∫£n ghi ƒë·ªçc ƒë∆∞·ª£c = ", len(df))

    df = process_incoming_df(df, encoded_features)

    predictions = model.predict(df)
    ratio  = np.count_nonzero(predictions)/len(predictions)
    print("Ratio = ", ratio)
    if ratio > hack_threshold:
        print("üõëHACKED!!!!!" * 20)
        return True
    else:
        print("‚úÖI AM SAFE!!!!!" * 20)
        return False


batch = []
start_time = time.time()
print("Start listening...")
while True:
    # ƒê·ªçc t·ª´ kafka
    records = consumer.poll(timeout_ms=timeout_readkafka)

    if not records:
        continue

    for topic_partition, messages in records.items():
        # ƒê∆∞a v√†o batch
        for message in messages:
            batch.append(message)
            # N·∫øu ƒë·ªß 10 b·∫£n ghi ho·∫∑c h·∫øt 1 ph√∫t th√¨ d·ª± ƒëo√°n
            if len(batch) >= batch_size or (time.time()-start_time)*1000>=batch_wait_time:
                # X·ª≠ l√Ω
                consumer.commit()
                process_batch(batch)

                batch = []
                start_time = time.time()

